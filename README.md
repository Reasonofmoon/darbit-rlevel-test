# CEFR-Based English Level Test System

## ğŸ¯ Overview

êµ­ì œ ê³µì¸ ê¸°ì¤€(CEFR: Common European Framework of Reference for Languages)ì— ë§ì¶˜ ê³¼í•™ì ì´ê³  ì²´ê³„ì ì¸ ì˜ì–´ ë ˆë²¨ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. í•™ìƒì˜ ì˜ì–´ ëŠ¥ë ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , 20ê°œ í•­ëª©ì˜ ìƒì„¸í•œ ë£¨ë¸Œë¦­ì„ í†µí•´ ê°•ì ê³¼ ì•½ì ì„ ë¶„ì„í•©ë‹ˆë‹¤.

---

## ğŸ“Š System Features

### 1. **5-Level CEFR Test Generation**
- **PRE-A1**: Basic beginner (K-1.5 grade reading)
- **A1**: Beginner (1.5-3.5 grade reading)
- **A2**: Elementary (3.5-5.0 grade reading)
- **B1**: Intermediate (5.0-7.0 grade reading)
- **B2**: Upper Intermediate (7.0+ grade reading)

### 2. **Comprehensive Assessment Sections**
ê° ë ˆë²¨ì˜ ì‹œí—˜ì€ ë‹¤ìŒ 5ê°œ ì˜ì—­ìœ¼ë¡œ êµ¬ì„±:

#### ğŸ“– Part 1: Reading Comprehension
- CEFR ë ˆë²¨ì— ë§ì¶˜ authentic texts
- Main idea, detail comprehension, inference questions
- ë ˆë²¨ë³„ 3-12ë¬¸í•­

#### ğŸ“š Part 2: Vocabulary
- Context-based vocabulary questions
- Collocations, word formation, precision
- ë ˆë²¨ë³„ 8-15ë¬¸í•­

#### ğŸ’¬ Part 3: Conversation
- Pragmatic appropriacy assessment
- Response selection in dialogues
- ë ˆë²¨ë³„ 5-8ë¬¸í•­

#### âœï¸ Part 4: Grammar
- Level-appropriate grammar points
- Sentence structure, tense, agreement
- ë ˆë²¨ë³„ 7-12ë¬¸í•­

#### ğŸ“ Part 5: Writing
- Task-based writing assessment
- Rubric-scored (0-4 scale)
- ë ˆë²¨ë³„ ë§ì¶¤í˜• ê³¼ì œ

### 3. **Scientific Item Design Principles**

ëª¨ë“  ë¬¸í•­ì€ ë‹¤ìŒ ì›ì¹™ì„ ì¤€ìˆ˜:

âœ… **Choice Length Balance Protocol v2.0**
- ì •ë‹µê³¼ ì˜¤ë‹µì˜ ê¸¸ì´ ê· í˜• ìœ ì§€
- êµ¬ì¡°ì  í¸í–¥ ì œê±°
- ì •ë‹µì´ í•­ìƒ ìµœì¥/ìµœë‹¨ì´ ì•„ë‹˜

âœ… **Answer Distribution Balance**
- A, B, C, D ì„ íƒì§€ ë¶„í¬ ê· ë“±
- ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì œê±°
- í†µê³„ì  ì‹ ë¢°ë„ ë³´ì¥

âœ… **Evidence-Based Difficulty Setting**
- ì˜ˆìƒ ì •ë‹µë¥  ì‚¬ì „ ì„¤ì •
- ë³€ë³„ë„ ì¡°ì ˆ
- ë§¤ë ¥ì  ì˜¤ë‹µ ì„¤ê³„

### 4. **20-Criteria Assessment Rubric**

ê° í•™ìƒì€ 20ê°œ í•­ëª©ìœ¼ë¡œ í‰ê°€:

**Reading (4 criteria)**
- R1: Main Idea Comprehension
- R2: Detail Comprehension
- R3: Inference Skills
- R4: Vocabulary in Context

**Vocabulary (4 criteria)**
- V1: Vocabulary Range
- V2: Vocabulary Precision
- V3: Collocations and Idioms
- V4: Word Formation

**Grammar (4 criteria)**
- G1: Sentence Structure
- G2: Verb Tenses
- G3: Subject-Verb Agreement
- G4: Articles and Determiners

**Conversation (4 criteria)**
- C1: Pragmatic Appropriacy
- C2: Turn-Taking and Interaction
- C3: Register and Formality
- C4: Conversational Strategies

**Writing (4 criteria)**
- W1: Task Achievement
- W2: Coherence and Cohesion
- W3: Grammatical Accuracy
- W4: Lexical Resource

ê° í•­ëª©ì€ 0-4ì  ì²™ë„ë¡œ í‰ê°€:
- **0**: Not Demonstrated
- **1**: Developing
- **2**: Adequate
- **3**: Proficient
- **4**: Exemplary

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/your-org/cefr-level-test.git
cd cefr-level-test
pip install -r requirements.txt
```

### Deploy to GitHub Pages

1. GitHub ì €ì¥ì†Œì—ì„œ `Settings > Pages`ì— ë“¤ì–´ê°€ Source ë¥¼ **GitHub Actions** ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
2. ê¸°ë³¸ ë¸Œëœì¹˜ ì´ë¦„ì´ `main`ì´ ì•„ë‹ˆë¼ë©´ `.github/workflows/pages.yml` ì˜ `branches: [main]` ë¶€ë¶„ì„ ë¸Œëœì¹˜ëª…ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
3. ì €ì¥ì†Œì— í‘¸ì‹œí•˜ë©´ ì›Œí¬í”Œë¡œìš°ê°€ ì‹¤í–‰ë˜ì–´ ìƒ˜í”Œ HTML/PDFë¥¼ ìƒì„±í•˜ê³  GitHub Pagesì— ìë™ ë°°í¬í•©ë‹ˆë‹¤. ë°°í¬ ê²°ê³¼ URLì€ Actions ë¡œê·¸ì™€ Pages ì„¤ì • í™”ë©´ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Usage

#### 1. Generate a Single Level Test

```bash
python3 main.py --mode generate --level A2 --output-dir ./outputs/
```

#### 2. Generate All Level Tests

```bash
python3 main.py --mode batch --output-dir ./outputs/
```

#### 3. Evaluate a Student Test

```python
from main import CEFRTestSystem

system = CEFRTestSystem(output_dir='./outputs')

# í•™ìƒ ë‹µì•ˆ
student_answers = {
    'R1': 'A', 'R2': 'B', 'R3': 'C',
    'V1': 'A', 'V2': 'B', 'V3': 'C',
    'G1': 'A', 'G2': 'B', 'G3': 'C',
    'C1': 'A', 'C2': 'B'
}

# ì •ë‹µ
correct_answers = {
    'R1': 'A', 'R2': 'C', 'R3': 'C',
    'V1': 'A', 'V2': 'B', 'V3': 'D',
    'G1': 'B', 'G2': 'B', 'G3': 'C',
    'C1': 'A', 'C2': 'C'
}

# í‰ê°€ ì‹¤í–‰
result_file = system.evaluate_test(
    level='A2',
    student_name='John Doe',
    student_answers=student_answers,
    correct_answers=correct_answers,
    writing_sample="Your writing sample here...",
    llm_feedback={'W1': {'score': 3, 'feedback': '...'}}
)
```

---

## ğŸ“ Output Files

### ì‹œí—˜ì§€ (Test Paper)
- **Format**: HTML (í”„ë¦°íŠ¸ ê°€ëŠ¥)
- **Location**: `outputs/tests/`
- **Content**: 
  - Student information section
  - All test sections with questions
  - Answer sheets
  - Writing space

### ì •ë‹µì§€ (Answer Key)
- **Format**: HTML
- **Location**: `outputs/answer_keys/`
- **Content**:
  - Quick reference answer grid
  - Detailed explanations
  - Difficulty indicators
  - Skill tags

### ê²°ê³¼ì§€ (Result Report)
- **Format**: HTML + PDF
- **Location**: `outputs/results/`
- **Content**:
  - Overall score and level determination
  - 20-criteria checklist with scores
  - Category breakdown chart
  - Strengths and weaknesses
  - Learning recommendations

### ë°ì´í„° (Test Data)
- **Format**: JSON
- **Location**: `outputs/tests/`
- **Content**: Structured test data for analysis

---

## ğŸ”§ Customization

### Adjust Question Counts

```python
custom_counts = {
    'reading': 10,
    'vocabulary': 15,
    'conversation': 10,
    'grammar': 15,
    'writing': 1
}

system.generate_test(level='B1', question_counts=custom_counts)
```

### Integrate LLM Feedback

ì‘ë¬¸ í‰ê°€ë¥¼ ìœ„í•´ LLM API ì—°ë™:

```python
# Example with OpenAI API
import openai

def get_writing_feedback(writing_sample, level):
    prompt = f"""
    Evaluate this {level} level writing sample using these 4 criteria:
    1. Task Achievement (0-4)
    2. Coherence and Cohesion (0-4)
    3. Grammatical Accuracy (0-4)
    4. Lexical Resource (0-4)
    
    Writing: {writing_sample}
    
    Return JSON format with scores and feedback.
    """
    
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    
    return json.loads(response.choices[0].message.content)

# Use in evaluation
llm_feedback = get_writing_feedback(writing_sample, 'A2')
system.evaluate_test(..., llm_feedback=llm_feedback)
```

---

## ğŸ“ˆ Scoring System

### Objective Sections
- **Multiple Choice**: 1 point per question
- **Automatic scoring**
- **Answer distribution analysis**

### Writing Section
- **Rubric-based**: 0-4 per criterion (4 criteria)
- **LLM or human evaluation**
- **Detailed feedback generation**

### Total Score Calculation

```
Total Score = Î£(Category Score Ã— Weight)

Weights:
- Reading: 30%
- Vocabulary: 20%
- Grammar: 20%
- Conversation: 15%
- Writing: 15%

Maximum: 80 points
```

### Level Determination

| Score Range | Level | Pass Threshold |
|-------------|-------|----------------|
| 0-30 | PRE-A1 | 24 |
| 31-45 | A1 | 36 |
| 46-58 | A2 | 48 |
| 59-70 | B1 | 60 |
| 71-80 | B2 | 72 |

---

## ğŸ¨ HTML Output Features

### Responsive Design
- Mobile-friendly layout
- Print-optimized
- Professional styling

### Interactive Elements
- Radio buttons for answer selection
- Visual score charts
- Color-coded feedback

### Accessibility
- Clear typography
- High contrast ratios
- Semantic HTML structure

---

## ğŸ”¬ Psychometric Quality

### Validity
- **Content Validity**: Aligned with CEFR descriptors
- **Construct Validity**: Measures intended skills
- **Face Validity**: Professionally designed items

### Reliability
- **Internal Consistency**: Balanced difficulty
- **Test-Retest**: Consistent level determination
- **Inter-Rater**: Standardized rubrics

### Fairness
- **No Cultural Bias**: Neutral content
- **Balanced Distractors**: Scientific design
- **Equal Opportunity**: Clear instructions

---

## ğŸ“š Reference Materials

### CEFR Official Documents
- [Council of Europe - CEFR](https://www.coe.int/en/web/common-european-framework-reference-languages)
- [CEFR Descriptors](https://www.coe.int/en/web/common-european-framework-reference-languages/table-1-cefr-3.3-common-reference-levels-global-scale)

### Lexile Framework
- Reading level alignment with US grade levels
- [Lexile Framework](https://lexile.com/)

### Assessment Standards
- ISO 29990: Learning services quality
- AERA Standards for Educational Testing

---

## ğŸ› ï¸ Technical Stack

- **Language**: Python 3.8+
- **Output**: HTML5, CSS3
- **Data Format**: JSON
- **PDF Generation**: ReportLab (bundled; no headless browser needed)
- **LLM Integration**: OpenAI API, Anthropic API (optional)

---

## ğŸ“ License

Â© 2024 CEFR Level Testing System. All rights reserved.

---

## ğŸ’¬ Support

For questions or support:
- Review the documentation
- Check example outputs
- Consult CEFR reference materials

---

## ğŸ”„ Version History

### v1.0.0 (2024-11-24)
- Initial release
- 5-level test generation (PRE-A1 to B2)
- 20-criteria assessment rubric
- HTML/PDF output support
- Comprehensive result reports

---

**Built with scientific rigor and educational expertise. ğŸ“**
